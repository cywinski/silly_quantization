{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from glob import glob\n",
        "import os\n",
        "from torchvision import transforms, models\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision.utils import make_grid\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "from collections import namedtuple\n",
        "from torchsummary import summary\n",
        "import time\n",
        "import copy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1icpVeSsajwn"
      },
      "source": [
        "# Download the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ANbMZUu9G419"
      },
      "outputs": [],
      "source": [
        "!gdown 1C0aYI36oCDHSJCY3O69xV2QwycdJ2Rvl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AGkEWM47FmR1"
      },
      "outputs": [],
      "source": [
        "!unzip city_seg.zip -d seg_data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cZ0pIuGGaotg"
      },
      "source": [
        "# Prepare dataset\n",
        "During training we use images with resolution 256x256 and masks 64x64 since the model outputs predictions of resolution 64x64. However, during final evaluation we calculate the metrics and visualize the results for masks resized to the original 256x256 size. \n",
        "\n",
        "For the training to be more efficient we store all images and masks in the object of dataset. This results in much more efficient loading of batch data during the training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EjihyefmN9Sx"
      },
      "outputs": [],
      "source": [
        "Label = namedtuple(\n",
        "    \"Label\",\n",
        "    [\n",
        "        \"name\",\n",
        "        \"id\",\n",
        "        \"trainId\",\n",
        "        \"category\",\n",
        "        \"categoryId\",\n",
        "        \"hasInstances\",\n",
        "        \"ignoreInEval\",\n",
        "        \"color\",\n",
        "    ],\n",
        ")\n",
        "\n",
        "labels = [\n",
        "    #       name                     id    trainId   category            catId     hasInstances   ignoreInEval   color\n",
        "    Label(\"unlabeled\", 0, 255, \"void\", 0, False, True, (0, 0, 0)),\n",
        "    Label(\"ego vehicle\", 1, 255, \"void\", 0, False, True, (0, 0, 0)),\n",
        "    Label(\"rectification border\", 2, 255, \"void\", 0, False, True, (0, 0, 0)),\n",
        "    Label(\"out of roi\", 3, 255, \"void\", 0, False, True, (0, 0, 0)),\n",
        "    Label(\"static\", 4, 255, \"void\", 0, False, True, (0, 0, 0)),\n",
        "    Label(\"dynamic\", 5, 255, \"void\", 0, False, True, (111, 74, 0)),\n",
        "    Label(\"ground\", 6, 255, \"void\", 0, False, True, (81, 0, 81)),\n",
        "    Label(\"road\", 7, 0, \"ground\", 1, False, False, (128, 64, 128)),\n",
        "    Label(\"sidewalk\", 8, 1, \"ground\", 8, False, False, (244, 35, 232)),\n",
        "    Label(\"parking\", 9, 255, \"ground\", 1, False, True, (250, 170, 160)),\n",
        "    Label(\"rail track\", 10, 255, \"ground\", 1, False, True, (230, 150, 140)),\n",
        "    Label(\"building\", 11, 2, \"construction\", 2, False, False, (70, 70, 70)),\n",
        "    Label(\"wall\", 12, 3, \"construction\", 2, False, False, (102, 102, 156)),\n",
        "    Label(\"fence\", 13, 4, \"construction\", 2, False, False, (190, 153, 153)),\n",
        "    Label(\"guard rail\", 14, 255, \"construction\", 2, False, True, (180, 165, 180)),\n",
        "    Label(\"bridge\", 15, 255, \"construction\", 2, False, True, (150, 100, 100)),\n",
        "    Label(\"tunnel\", 16, 255, \"construction\", 2, False, True, (150, 120, 90)),\n",
        "    Label(\"pole\", 17, 5, \"object\", 3, False, False, (153, 153, 153)),\n",
        "    Label(\"polegroup\", 18, 255, \"object\", 3, False, True, (153, 153, 153)),\n",
        "    Label(\"traffic light\", 19, 6, \"object\", 3, False, False, (250, 170, 30)),\n",
        "    Label(\"traffic sign\", 20, 7, \"object\", 3, False, False, (220, 220, 0)),\n",
        "    Label(\"vegetation\", 21, 8, \"nature\", 4, False, False, (107, 142, 35)),\n",
        "    Label(\"terrain\", 22, 9, \"nature\", 4, False, False, (152, 251, 152)),\n",
        "    Label(\"sky\", 23, 10, \"sky\", 5, False, False, (70, 130, 180)),\n",
        "    Label(\"person\", 24, 11, \"human\", 6, True, False, (220, 20, 60)),\n",
        "    Label(\"rider\", 25, 12, \"human\", 6, True, False, (255, 0, 0)),\n",
        "    Label(\"car\", 26, 13, \"vehicle\", 7, True, False, (0, 0, 142)),\n",
        "    Label(\"truck\", 27, 14, \"vehicle\", 7, True, False, (0, 0, 70)),\n",
        "    Label(\"bus\", 28, 15, \"vehicle\", 7, True, False, (0, 60, 100)),\n",
        "    Label(\"caravan\", 29, 255, \"vehicle\", 7, True, True, (0, 0, 90)),\n",
        "    Label(\"trailer\", 30, 255, \"vehicle\", 7, True, True, (0, 0, 110)),\n",
        "    Label(\"train\", 31, 16, \"vehicle\", 7, True, False, (0, 80, 100)),\n",
        "    Label(\"motorcycle\", 32, 17, \"vehicle\", 7, True, False, (0, 0, 230)),\n",
        "    Label(\"bicycle\", 33, 18, \"vehicle\", 7, True, False, (119, 11, 32)),\n",
        "    Label(\"license plate\", 34, 19, \"vehicle\", 7, False, True, (0, 0, 142)),\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G0ounX_pXVfZ"
      },
      "outputs": [],
      "source": [
        "train_filepath = \"seg_data/cityscapes_data/cityscapes_data/train\"\n",
        "val_filepath = \"seg_data/cityscapes_data/cityscapes_data/val\"\n",
        "TRAIN_IMAGE_SIZE = (256, 256)\n",
        "TRAIN_MASK_SIZE = (64, 64)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mFWuQXErGp2Q"
      },
      "outputs": [],
      "source": [
        "class CityscapesDataset(Dataset):\n",
        "    def __init__(\n",
        "        self,\n",
        "        root,\n",
        "        img_transform=None,\n",
        "        mask_transform=None,\n",
        "        img_size=(256, 256),\n",
        "        mask_size=(64, 64),\n",
        "    ):\n",
        "        self.files = glob(os.path.join(root, \"*.jpg\"))\n",
        "        self.id2color = {label.id: np.asarray(label.color) for label in labels}\n",
        "        self.num_classes = len(self.id2color)\n",
        "        self.img_transform = img_transform\n",
        "        self.mask_transform = mask_transform\n",
        "\n",
        "        self.images = []\n",
        "        self.masks = []\n",
        "\n",
        "        for f in tqdm(self.files, desc=\"Building Dataset: \", total=len(self.files)):\n",
        "            image, mask = self._image_mask_split(f, img_size, mask_size)\n",
        "            mask = self._find_closest_labels_vectorized(mask)\n",
        "            mask = np.eye(self.num_classes)[mask]\n",
        "            mask = mask.astype(np.uint8)\n",
        "            mask = np.transpose(mask, (2, 0, 1))\n",
        "            mask = torch.from_numpy(mask)\n",
        "            self.images.append(image)\n",
        "            self.masks.append(mask)\n",
        "\n",
        "    def _image_mask_split(self, filename, image_size=(256, 256), mask_size=(64, 64)):\n",
        "        with Image.open(filename) as img:\n",
        "            image, mask = img.crop([0, 0, 256, 256]), img.crop([256, 0, 512, 256])\n",
        "\n",
        "            image = image.resize((image_size))\n",
        "            mask = mask.resize((mask_size))\n",
        "\n",
        "            image = np.array(image)\n",
        "            mask = np.array(mask)\n",
        "\n",
        "            return image, mask\n",
        "\n",
        "    def _find_closest_labels_vectorized(\n",
        "        self, mask\n",
        "    ):  # 'mapping' is a RGB color tuple to categorical number dictionary\n",
        "        closest_distance = np.full([mask.shape[0], mask.shape[1]], 10000)\n",
        "        closest_category = np.full([mask.shape[0], mask.shape[1]], 0)\n",
        "\n",
        "        for id, color in self.id2color.items():  # iterate over every color mapping\n",
        "            dist = np.sqrt(np.linalg.norm(mask - color.reshape([1, 1, -1]), axis=-1))\n",
        "            is_closer = closest_distance > dist\n",
        "            closest_distance = np.where(is_closer, dist, closest_distance)\n",
        "            closest_category = np.where(is_closer, id, closest_category)\n",
        "\n",
        "        return closest_category\n",
        "\n",
        "    def mask2color(self, mask):\n",
        "        # Map each integer to its corresponding color\n",
        "        # mask should be NxN shape\n",
        "        output_mask = torch.zeros((3, mask.shape[0], mask.shape[1]))\n",
        "        for id, color in self.id2color.items():\n",
        "            color_mask = mask == id\n",
        "            for channel, color_value in enumerate(color):\n",
        "                output_mask[channel, color_mask] = color_value\n",
        "        return output_mask\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.files)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        rgb_image = self.images[idx]\n",
        "        mask = self.masks[idx]\n",
        "\n",
        "        if self.img_transform is not None:\n",
        "            rgb_image = self.img_transform(rgb_image)\n",
        "        if self.mask_transform is not None:\n",
        "            mask = self.mask_transform(mask)\n",
        "\n",
        "        return rgb_image, mask"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KA9LVO-oXTkm"
      },
      "outputs": [],
      "source": [
        "CITYSCAPES_MEAN = [0.485, 0.456, 0.406]\n",
        "CITYSCAPES_STD = [0.229, 0.224, 0.225]\n",
        "\n",
        "img_transform = transforms.Compose(\n",
        "    [transforms.ToTensor(), transforms.Normalize(CITYSCAPES_MEAN, CITYSCAPES_STD)]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CC7O2y0yHl36"
      },
      "outputs": [],
      "source": [
        "train_dataset = CityscapesDataset(\n",
        "    train_filepath,\n",
        "    img_transform=img_transform,\n",
        "    img_size=TRAIN_IMAGE_SIZE,\n",
        "    mask_size=TRAIN_MASK_SIZE,\n",
        ")\n",
        "val_dataset = CityscapesDataset(\n",
        "    val_filepath,\n",
        "    img_transform=img_transform,\n",
        "    img_size=TRAIN_IMAGE_SIZE,\n",
        "    mask_size=TRAIN_MASK_SIZE,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lq62q2QlHs3t"
      },
      "outputs": [],
      "source": [
        "len(train_dataset), len(val_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lMU6Fp4FIzDM"
      },
      "outputs": [],
      "source": [
        "sample_img, sample_gt_mask = train_dataset[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OZJeFcumNfe1"
      },
      "outputs": [],
      "source": [
        "sample_img.shape, sample_gt_mask.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y0jI0MaAWSwq"
      },
      "outputs": [],
      "source": [
        "def resize_tensor(inputs, target_size):\n",
        "    inputs = torch.nn.functional.interpolate(inputs, size=target_size)\n",
        "    return inputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xa0vph239h83"
      },
      "outputs": [],
      "source": [
        "def show_tensor_images(image, gt_mask, pred_mask=None):\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.subplot(1, 3, 1)\n",
        "\n",
        "    plt.imshow(make_grid([image], normalize=True).permute(1, 2, 0))\n",
        "    plt.axis(\"off\")\n",
        "    plt.title(\"Image\")\n",
        "\n",
        "    plt.subplot(1, 3, 2)\n",
        "    gt_mask = np.transpose(gt_mask.numpy(), (1, 2, 0)) / 255\n",
        "    plt.imshow(gt_mask)\n",
        "    plt.axis(\"off\")\n",
        "    plt.title(\"GT Mask\")\n",
        "\n",
        "    if pred_mask is not None:\n",
        "        plt.subplot(1, 3, 3)\n",
        "        pred_mask = np.transpose(pred_mask.numpy(), (1, 2, 0)) / 255\n",
        "        plt.imshow(pred_mask)\n",
        "        plt.axis(\"off\")\n",
        "        plt.title(\"Pred Mask\")\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DFfBZXkgvacz"
      },
      "source": [
        "# Define the model\n",
        "As an encoder we use 3 middle layers from the `MobileNetV2`. As decoder we use the `FFNet` with variant C that has the least number of parameters from all three variants."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m5apwPcPrO3V"
      },
      "outputs": [],
      "source": [
        "BN_MOMENTUM = 0.1\n",
        "gpu_up_kwargs = {\"mode\": \"bilinear\", \"align_corners\": True}\n",
        "mobile_up_kwargs = {\"mode\": \"nearest\"}\n",
        "relu_inplace = True\n",
        "NUM_CLASSES = len(labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6e9tL_U5qwGH"
      },
      "outputs": [],
      "source": [
        "class ConvBNReLU(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        in_chan,\n",
        "        out_chan,\n",
        "        ks=3,\n",
        "        stride=1,\n",
        "        padding=1,\n",
        "        activation=nn.ReLU,\n",
        "        *args,\n",
        "        **kwargs,\n",
        "    ):\n",
        "        super(ConvBNReLU, self).__init__()\n",
        "        layers = [\n",
        "            nn.Conv2d(\n",
        "                in_chan,\n",
        "                out_chan,\n",
        "                kernel_size=ks,\n",
        "                stride=stride,\n",
        "                padding=padding,\n",
        "                bias=False,\n",
        "            ),\n",
        "            nn.BatchNorm2d(out_chan, momentum=BN_MOMENTUM),\n",
        "        ]\n",
        "        if activation:\n",
        "            layers.append(activation(inplace=relu_inplace))\n",
        "        self.layers = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.layers(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kvtGV8zFsSqi"
      },
      "outputs": [],
      "source": [
        "class AdapterConv(nn.Module):\n",
        "    def __init__(\n",
        "        self, in_channels=[256, 512, 1024, 2048], out_channels=[64, 128, 256, 512]\n",
        "    ):\n",
        "        super(AdapterConv, self).__init__()\n",
        "        assert len(in_channels) == len(\n",
        "            out_channels\n",
        "        ), \"Number of input and output branches should match\"\n",
        "        self.adapter_conv = nn.ModuleList()\n",
        "\n",
        "        for k in range(len(in_channels)):\n",
        "            self.adapter_conv.append(\n",
        "                ConvBNReLU(in_channels[k], out_channels[k], ks=1, stride=1, padding=0),\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = []\n",
        "        for k in range(len(self.adapter_conv)):\n",
        "            out.append(self.adapter_conv[k](x[k]))\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_e6bBQEduI3J"
      },
      "outputs": [],
      "source": [
        "class UpsampleCat(nn.Module):\n",
        "    def __init__(self, upsample_kwargs=gpu_up_kwargs):\n",
        "        super(UpsampleCat, self).__init__()\n",
        "        self._up_kwargs = upsample_kwargs\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"Upsample and concatenate feature maps.\"\"\"\n",
        "        assert isinstance(x, list) or isinstance(x, tuple)\n",
        "        # print(self._up_kwargs)\n",
        "        x0 = x[0]\n",
        "        _, _, H, W = x0.size()\n",
        "        for i in range(1, len(x)):\n",
        "            x0 = torch.cat([x0, F.interpolate(x[i], (H, W), **self._up_kwargs)], dim=1)\n",
        "        return x0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JFf-bwYcu0Dr"
      },
      "outputs": [],
      "source": [
        "class UpBranch(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        in_channels=[64, 128, 256, 512],\n",
        "        out_channels=[128, 128, 128, 128],\n",
        "        upsample_kwargs=gpu_up_kwargs,\n",
        "    ):\n",
        "        super(UpBranch, self).__init__()\n",
        "\n",
        "        self._up_kwargs = upsample_kwargs\n",
        "\n",
        "        self.fam_32_sm = ConvBNReLU(\n",
        "            in_channels[3], out_channels[3], ks=3, stride=1, padding=1\n",
        "        )\n",
        "        self.fam_32_up = ConvBNReLU(\n",
        "            in_channels[3], in_channels[2], ks=1, stride=1, padding=0\n",
        "        )\n",
        "        self.fam_16_sm = ConvBNReLU(\n",
        "            in_channels[2], out_channels[2], ks=3, stride=1, padding=1\n",
        "        )\n",
        "        self.fam_16_up = ConvBNReLU(\n",
        "            in_channels[2], in_channels[1], ks=1, stride=1, padding=0\n",
        "        )\n",
        "        self.fam_8_sm = ConvBNReLU(\n",
        "            in_channels[1], out_channels[1], ks=3, stride=1, padding=1\n",
        "        )\n",
        "        self.fam_8_up = ConvBNReLU(\n",
        "            in_channels[1], in_channels[0], ks=1, stride=1, padding=0\n",
        "        )\n",
        "        self.fam_4 = ConvBNReLU(\n",
        "            in_channels[0], out_channels[0], ks=3, stride=1, padding=1\n",
        "        )\n",
        "\n",
        "        self.high_level_ch = sum(out_channels)\n",
        "        self.out_channels = out_channels\n",
        "\n",
        "    def forward(self, x):\n",
        "        feat4, feat8, feat16, feat32 = x\n",
        "\n",
        "        smfeat_32 = self.fam_32_sm(feat32)\n",
        "        upfeat_32 = self.fam_32_up(feat32)\n",
        "\n",
        "        _, _, H, W = feat16.size()\n",
        "        x = F.interpolate(upfeat_32, (H, W), **self._up_kwargs) + feat16\n",
        "        smfeat_16 = self.fam_16_sm(x)\n",
        "        upfeat_16 = self.fam_16_up(x)\n",
        "\n",
        "        _, _, H, W = feat8.size()\n",
        "        x = F.interpolate(upfeat_16, (H, W), **self._up_kwargs) + feat8\n",
        "        smfeat_8 = self.fam_8_sm(x)\n",
        "        upfeat_8 = self.fam_8_up(x)\n",
        "\n",
        "        _, _, H, W = feat4.size()\n",
        "        smfeat_4 = self.fam_4(\n",
        "            F.interpolate(upfeat_8, (H, W), **self._up_kwargs) + feat4\n",
        "        )\n",
        "\n",
        "        return smfeat_4, smfeat_8, smfeat_16, smfeat_32"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pjX_ws4fk0Pp"
      },
      "outputs": [],
      "source": [
        "class MobileNetV2C(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MobileNetV2C, self).__init__()\n",
        "        mobilenet = models.mobilenet_v2(\n",
        "            weights=models.MobileNet_V2_Weights.IMAGENET1K_V1\n",
        "        )\n",
        "        self.stem = nn.Sequential(\n",
        "            ConvBNReLU(3, 16, ks=3, stride=2, padding=1),\n",
        "            ConvBNReLU(16, 16, ks=3, stride=2, padding=1),\n",
        "            ConvBNReLU(16, 16, ks=3, stride=1, padding=1),\n",
        "        )\n",
        "        self.layer1 = mobilenet.features[2]\n",
        "        self.layer2 = mobilenet.features[3]\n",
        "        self.layer3 = mobilenet.features[4]\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.stem(x)  # 16, 64, 64\n",
        "        x1 = self.layer1(x)  # 24, 32, 32\n",
        "        x2 = self.layer2(x1)  # 24, 32, 32\n",
        "        x3 = self.layer3(x2)  # 32, 16, 16\n",
        "        return x, x1, x2, x3\n",
        "\n",
        "\n",
        "class EfficientNetB0C(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(EfficientNetB0C, self).__init__()\n",
        "        efficientnet = models.efficientnet_b0(\n",
        "            weights=models.EfficientNet_B0_Weights.IMAGENET1K_V1\n",
        "        )\n",
        "        self.stem = nn.Sequential(\n",
        "            ConvBNReLU(3, 40, ks=3, stride=2, padding=1),\n",
        "            ConvBNReLU(40, 40, ks=3, stride=2, padding=1),\n",
        "            ConvBNReLU(40, 40, ks=3, stride=1, padding=1),\n",
        "        )\n",
        "        self.layer1 = efficientnet.features[4]\n",
        "        self.layer2 = efficientnet.features[5]\n",
        "        self.layer3 = efficientnet.features[6]\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.stem(x)  # 16, 64, 64\n",
        "        x1 = self.layer1(x)  # 24, 32, 32\n",
        "        x2 = self.layer2(x1)  # 24, 32, 32\n",
        "        x3 = self.layer3(x2)  # 32, 16, 16\n",
        "        return x, x1, x2, x3\n",
        "\n",
        "\n",
        "class UpC(nn.Module):\n",
        "    def __init__(self, in_channels, base_channels):\n",
        "        super(UpC, self).__init__()\n",
        "        self.layers = nn.Sequential(\n",
        "            AdapterConv(in_channels, base_channels),\n",
        "            UpBranch(\n",
        "                in_channels=base_channels,\n",
        "                out_channels=[128, 16, 16, 16],\n",
        "                upsample_kwargs=gpu_up_kwargs,\n",
        "            ),\n",
        "            UpsampleCat(upsample_kwargs=gpu_up_kwargs),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.layers(x)\n",
        "\n",
        "\n",
        "class SegHeadC(nn.Module):\n",
        "    def __init__(self, in_channels, num_classes=NUM_CLASSES):\n",
        "        super(SegHeadC, self).__init__()\n",
        "        self.layers = nn.Sequential(\n",
        "            ConvBNReLU(in_channels, 128, ks=3, stride=1, padding=1),\n",
        "            nn.Conv2d(128, num_classes, kernel_size=1, stride=1, padding=0),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.layers(x)\n",
        "\n",
        "\n",
        "class FFNetC(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(FFNetC, self).__init__()\n",
        "        self.encoder = EfficientNetB0C()\n",
        "        self.decoder = UpC([40, 80, 112, 192], [128, 16, 16, 16])\n",
        "        self.head = SegHeadC(176, NUM_CLASSES)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.encoder(x)\n",
        "        x = self.decoder(x)\n",
        "        x = self.head(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Sanity check: validate if model outputs expected shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6_GG543ozrHz"
      },
      "outputs": [],
      "source": [
        "output = FFNetC()(sample_img.unsqueeze(0))\n",
        "output.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PtB_i5ac1uel"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zR9ly9PmESnb"
      },
      "source": [
        "Calculate mIoU and frequency weighted IoU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "40oKrU81MzrV"
      },
      "outputs": [],
      "source": [
        "def calculate_frequency_weighted_iou(gt_masks, pred_masks, num_classes):\n",
        "    total_pixels = np.prod(gt_masks.shape)\n",
        "    iou_scores = []\n",
        "    class_frequencies = []\n",
        "\n",
        "    for class_id in range(num_classes):\n",
        "        # Create binary masks for the current class\n",
        "        gt_class_mask = (gt_masks == class_id).astype(bool)\n",
        "        pred_class_mask = (pred_masks == class_id).astype(bool)\n",
        "\n",
        "        # Calculate intersection and union\n",
        "        intersection = np.logical_and(gt_class_mask, pred_class_mask)\n",
        "        union = np.logical_or(gt_class_mask, pred_class_mask)\n",
        "\n",
        "        # Calculate the intersection and union areas\n",
        "        intersection_area = np.sum(intersection)\n",
        "        union_area = np.sum(union)\n",
        "\n",
        "        # Calculate IoU for the current class\n",
        "        if union_area == 0:\n",
        "            # Avoid division by zero; if there's no ground truth, consider the prediction perfect if it is also empty\n",
        "            iou = 1.0 if intersection_area == 0 else 0\n",
        "        else:\n",
        "            iou = intersection_area / union_area\n",
        "        iou_scores.append(iou)\n",
        "\n",
        "        # Calculate frequency of each class\n",
        "        class_frequency = np.sum(gt_class_mask) / total_pixels\n",
        "        class_frequencies.append(class_frequency)\n",
        "\n",
        "    # Calculate FWIoU\n",
        "    fwiou = np.sum(np.array(iou_scores) * np.array(class_frequencies))\n",
        "    return iou_scores, class_frequencies, fwiou"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ngiy4093EXBb"
      },
      "source": [
        "Hyperparameters of training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qLv8bE1F4WGa"
      },
      "outputs": [],
      "source": [
        "BATCH_SIZE = 8\n",
        "LR = 3e-4\n",
        "EPOCHS = 50\n",
        "WEIGHT_DECAY = 5e-5\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f5MJiShP57Ik"
      },
      "outputs": [],
      "source": [
        "model = FFNetC()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tG88KkUI5dHX"
      },
      "outputs": [],
      "source": [
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE)\n",
        "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n",
        "lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, EPOCHS)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(summary(model, val_dataset[0][0].shape))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6xfxz8DDZDPe"
      },
      "source": [
        "During training we print losses and mIoU for 64x64 masks. After the training we additionally plot the loss curves and calculate the final metrics for 256x256 masks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E0cLjBqr5iOV"
      },
      "outputs": [],
      "source": [
        "train_losses = []\n",
        "val_losses = []\n",
        "ious = []\n",
        "fwious = []\n",
        "best_model_weights = None\n",
        "best_val_loss = float(\"inf\")\n",
        "model.to(DEVICE)\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    model.train()\n",
        "\n",
        "    epoch_train_loss = 0.0\n",
        "    epoch_val_loss = 0.0\n",
        "    for imgs, masks in train_loader:\n",
        "        imgs = imgs.to(DEVICE)\n",
        "        masks = masks.to(DEVICE).float()\n",
        "        outputs = model(imgs)\n",
        "\n",
        "        loss = criterion(outputs, masks)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        epoch_train_loss += loss.item()\n",
        "\n",
        "    lr_scheduler.step()\n",
        "\n",
        "    model.eval()\n",
        "    iou = 0.0\n",
        "    fwiou = 0.0\n",
        "    for imgs, masks in val_loader:\n",
        "        imgs = imgs.to(DEVICE)\n",
        "        masks = masks.to(DEVICE).float()\n",
        "        outputs = model(imgs)\n",
        "        loss = criterion(outputs, masks)\n",
        "\n",
        "        preds = torch.argmax(outputs, dim=1).cpu().numpy()\n",
        "        gt = torch.argmax(masks, dim=1).cpu().numpy()\n",
        "\n",
        "        iou_scores, class_frequencies, _fwiou = calculate_frequency_weighted_iou(\n",
        "            gt.flatten(), preds.flatten(), NUM_CLASSES\n",
        "        )\n",
        "\n",
        "        epoch_val_loss += loss.item()\n",
        "        iou += np.mean(iou_scores)\n",
        "        fwiou += _fwiou\n",
        "\n",
        "    iou = iou / len(val_loader)\n",
        "    fwiou = fwiou / len(val_loader)\n",
        "    epoch_train_loss = epoch_train_loss / len(train_loader)\n",
        "    epoch_val_loss = epoch_val_loss / len(val_loader)\n",
        "\n",
        "    print(\n",
        "        f\"[Epoch {epoch + 1} / {EPOCHS}] [train loss: {epoch_train_loss:.3f}] [val loss: {epoch_val_loss:.3f}] [val mIoU: {iou:.3f}] [val fwIoU: {fwiou:.3f}]\"\n",
        "    )\n",
        "\n",
        "    train_losses.append(epoch_train_loss)\n",
        "    val_losses.append(epoch_val_loss)\n",
        "    ious.append(iou)\n",
        "    fwious.append(fwiou)\n",
        "    if epoch_val_loss < best_val_loss:\n",
        "        best_val_loss = epoch_val_loss\n",
        "        best_model_weights = model.state_dict().copy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YWZ_0GKWnGYu"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(14, 5))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(train_losses, label=\"train\", marker=\"o\")\n",
        "plt.plot(val_losses, label=\"val\", marker=\"o\")\n",
        "plt.xlabel(\"epoch\")\n",
        "plt.title(\"loss\")\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(ious, marker=\"o\", label=\"mean\")\n",
        "plt.plot(fwious, marker=\"o\", label=\"freq weighted\")\n",
        "plt.xlabel(\"epoch\")\n",
        "plt.title(\"IoU\")\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AFOTguPZZPnp"
      },
      "source": [
        "# Evaluation of the original FP32 model\n",
        "Calculate the metrics and visualize the predictions for 256x256 masks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "test_dataset = CityscapesDataset(\n",
        "    val_filepath, img_transform=img_transform, img_size=(256, 256), mask_size=(256, 256)\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BiCJNhAJcYG-"
      },
      "outputs": [],
      "source": [
        "if os.path.exists(\"model.pt\"):\n",
        "    model.load_state_dict(torch.load(\"model.pt\", map_location=DEVICE))\n",
        "else:\n",
        "    model.load_state_dict(best_model_weights)\n",
        "    torch.save(model.state_dict(), \"model.pt\")\n",
        "model.eval()\n",
        "model.to(DEVICE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def evaluate(model, dataset, verbose=False, device=DEVICE):\n",
        "    fwIoU = 0.0\n",
        "    mIoU = 0.0\n",
        "    model = model.to(device)\n",
        "    for i in tqdm(range(len(dataset))):\n",
        "        img, mask = dataset[i]\n",
        "        out = model(img.unsqueeze(0).to(device))  # 35x64x64\n",
        "        preds = torch.argmax(out, dim=1)[0]  # 64x64\n",
        "        preds = preds.view(1, 1, preds.shape[0], preds.shape[1]).float()\n",
        "        preds = resize_tensor(preds, (256, 256))  # 256x256\n",
        "        colored_out = test_dataset.mask2color(preds[0, 0, :, :])  # 3x256x256\n",
        "        colored_gt = test_dataset.mask2color(torch.argmax(mask, dim=0))  # 3x256x256\n",
        "\n",
        "        preds = preds[0, 0, :, :].cpu().numpy()\n",
        "        gt = torch.argmax(mask, dim=0).cpu().numpy()\n",
        "\n",
        "        iou_scores, class_frequencies, fwiou = calculate_frequency_weighted_iou(\n",
        "            gt.flatten(), preds.flatten(), NUM_CLASSES\n",
        "        )\n",
        "\n",
        "        mIoU += np.mean(iou_scores)\n",
        "        fwIoU += fwiou\n",
        "\n",
        "        if verbose and i % 50 == 0:\n",
        "            show_tensor_images(img, colored_gt, colored_out)\n",
        "\n",
        "    fwIoU = fwIoU / len(dataset)\n",
        "    mIoU = mIoU / len(dataset)\n",
        "\n",
        "    return mIoU, fwIoU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Evaluation of the original FP32 model ...\")\n",
        "mIoU, fwIoU = evaluate(model.to(DEVICE), test_dataset, verbose=True, device=DEVICE)\n",
        "print(f\"Validation mIoU: {mIoU:.2f}\")\n",
        "print(f\"Validation Frequence Weighted IoU: {fwIoU:.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Quantization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%pip install git+https://github.com/alibaba/TinyNeuralNetwork.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from tinynn.graph.quantization.quantizer import PostQuantizer\n",
        "from tinynn.util.train_util import AverageMeter\n",
        "from tinynn.graph.tracer import model_tracer\n",
        "from tinynn.graph.quantization.algorithm.cross_layer_equalization import (\n",
        "    cross_layer_equalize,\n",
        ")\n",
        "from tinynn.graph.quantization.fake_quantize import set_ptq_fake_quantize"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def plot_output_channel(X):\n",
        "    X = X.detach().cpu()\n",
        "    C = X.shape[1]\n",
        "    # Create a figure and a set of subplots\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 5))  # 1 row, 2 columns\n",
        "    ranges = torch.zeros((X.size(1), 2))  # Two columns for min and max\n",
        "    for i in range(X.size(1)):  # Iterate over channels\n",
        "        channel_data = X[:, i, :, :].flatten()  # Flatten the spatial dimensions\n",
        "        ranges[i, 0] = torch.min(channel_data)  # Min value for this channel\n",
        "        ranges[i, 1] = torch.max(channel_data)  # Max value for this channel\n",
        "\n",
        "    # Convert the ranges to a format suitable for box plot\n",
        "    ranges = ranges.numpy()\n",
        "\n",
        "    # Create the box plot\n",
        "    ax1.boxplot(ranges.transpose(), positions=range(1, C + 1), showfliers=False)\n",
        "\n",
        "    ax1.set_title(\"Range of values for each output channel\")\n",
        "\n",
        "    ax2.hist(X.flatten().detach().numpy(), bins=100)\n",
        "    # Plot data on the second subplot\n",
        "    ax2.set_title(\"Distribution\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def calibrate(model, train_loader, max_iteration=None, device=DEVICE):\n",
        "    \"\"\"Calibrates the fake-quantized model\n",
        "\n",
        "    Args:\n",
        "        model: The model to be validated\n",
        "    \"\"\"\n",
        "\n",
        "    model.eval()\n",
        "    model = model.to(device)\n",
        "\n",
        "    avg_batch_time = AverageMeter()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        end = time.time()\n",
        "        for i, (imgs, _) in enumerate(train_loader):\n",
        "            if max_iteration is not None and i >= max_iteration:\n",
        "                break\n",
        "\n",
        "            imgs = imgs.to(device)\n",
        "            model(imgs)\n",
        "\n",
        "            # measure elapsed time\n",
        "            avg_batch_time.update(time.time() - end)\n",
        "            end = time.time()\n",
        "\n",
        "            if i % 10 == 0:\n",
        "                print(\n",
        "                    f\"Calibrate: [{i}/{len(train_loader)}]\\tTime {avg_batch_time.avg:.5f}\\t\"\n",
        "                )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Post-training quantization (PTQ)\n",
        "\n",
        "Post-training quantization (PTQ) algorithms take a pre-trained FP32 network and convert it directly into a fixed-point network without the need for the original training pipeline. These methods can be data-free or may require a small calibration set, which is often readily available."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "next(iter(model.parameters()))[0].dtype"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### PTQ per tensor without CLE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "with model_tracer():\n",
        "    model_copy = copy.deepcopy(model).eval()\n",
        "    model_copy = model.to(DEVICE)\n",
        "    dummy_input = torch.randn(1, 3, 256, 256)\n",
        "    # model_copy = cross_layer_equalize(model_copy, dummy_input, DEVICE)\n",
        "\n",
        "    # More information for QATQuantizer initialization, see `examples/quantization/qat.py`.\n",
        "    # We set 'override_qconfig_func' when initializing QATQuantizer to use fake-quantize to do post quantization.\n",
        "    quantizer = PostQuantizer(\n",
        "        model_copy,\n",
        "        dummy_input,\n",
        "        work_dir=\"out\",\n",
        "        config={\n",
        "            \"asymmetric\": True,\n",
        "            \"backend\": \"qnnpack\",\n",
        "            \"disable_requantization_for_cat\": True,\n",
        "            \"per_tensor\": True,\n",
        "            \"override_qconfig_func\": set_ptq_fake_quantize,\n",
        "        },\n",
        "    )\n",
        "    ptq_model = quantizer.quantize()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "ptq_model.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "activation = {}\n",
        "ptq_model = ptq_model.to(\"cpu\")\n",
        "\n",
        "\n",
        "def getActivation(name):\n",
        "    # the hook signature\n",
        "    def hook(model, input, output):\n",
        "        activation[name] = output.detach()\n",
        "\n",
        "    return hook\n",
        "\n",
        "\n",
        "# register forward hooks on the layers of choice\n",
        "h1 = ptq_model.encoder_stem_0_layers_0.register_forward_hook(\n",
        "    getActivation(\"encoder_stem_0_layers_0\")\n",
        ")\n",
        "\n",
        "activation_list = []\n",
        "for imgs, _ in tqdm(val_loader):\n",
        "    out = ptq_model(imgs)\n",
        "    activation_list.append(activation[\"encoder_stem_0_layers_0\"])\n",
        "h1.remove()\n",
        "activations = torch.cat(activation_list[:-1], dim=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plot_output_channel(activations)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "ptq_model = ptq_model.to(DEVICE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Post quantization calibration\n",
        "ptq_model.apply(torch.quantization.disable_fake_quant)\n",
        "ptq_model.apply(torch.quantization.enable_observer)\n",
        "calibrate(ptq_model, train_loader, max_iteration=100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Disable observer and enable fake quantization to validate model with quantization error\n",
        "ptq_model.apply(torch.quantization.disable_observer)\n",
        "ptq_model.apply(torch.quantization.enable_fake_quant)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "with torch.no_grad():\n",
        "    ptq_model.eval()\n",
        "    ptq_model.cpu()\n",
        "\n",
        "    # The step below converts the model to an actual quantized model, which uses the quantized kernels.\n",
        "    ptq_model = quantizer.convert(ptq_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "ptq_model.encoder_stem_0_layers_0.weight().dtype"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Evaluation of the quantized model (PTQ) per tensor ...\")\n",
        "mIoU, fwIoU = evaluate(ptq_model, test_dataset, verbose=False, device=\"cpu\")\n",
        "print(f\"Validation mIoU: {mIoU:.2f}\")\n",
        "print(f\"Validation Frequence Weighted IoU: {fwIoU:.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### PTQ per tensor with CLE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "with model_tracer():\n",
        "    model_copy = copy.deepcopy(model).eval()\n",
        "    model_copy = model.to(DEVICE)\n",
        "    dummy_input = torch.randn(1, 3, 256, 256)\n",
        "    model_copy = cross_layer_equalize(model_copy, dummy_input, DEVICE, cle_iters=10)\n",
        "\n",
        "    # More information for QATQuantizer initialization, see `examples/quantization/qat.py`.\n",
        "    # We set 'override_qconfig_func' when initializing QATQuantizer to use fake-quantize to do post quantization.\n",
        "    quantizer = PostQuantizer(\n",
        "        model_copy,\n",
        "        dummy_input,\n",
        "        work_dir=\"out\",\n",
        "        config={\n",
        "            \"asymmetric\": True,\n",
        "            \"backend\": \"qnnpack\",\n",
        "            \"disable_requantization_for_cat\": True,\n",
        "            \"per_tensor\": True,\n",
        "            \"override_qconfig_func\": set_ptq_fake_quantize,\n",
        "        },\n",
        "    )\n",
        "    ptq_model = quantizer.quantize()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "ptq_model.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "activation = {}\n",
        "ptq_model = ptq_model.to(\"cpu\")\n",
        "\n",
        "\n",
        "def getActivation(name):\n",
        "    # the hook signature\n",
        "    def hook(model, input, output):\n",
        "        activation[name] = output.detach()\n",
        "\n",
        "    return hook\n",
        "\n",
        "\n",
        "# register forward hooks on the layers of choice\n",
        "h1 = ptq_model.encoder_stem_0_layers_0.register_forward_hook(\n",
        "    getActivation(\"encoder_stem_0_layers_0\")\n",
        ")\n",
        "\n",
        "activation_list = []\n",
        "for imgs, _ in tqdm(val_loader):\n",
        "    out = ptq_model(imgs)\n",
        "    activation_list.append(activation[\"encoder_stem_0_layers_0\"])\n",
        "h1.remove()\n",
        "activations = torch.cat(activation_list[:-1], dim=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plot_output_channel(activations)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "ptq_model = ptq_model.to(DEVICE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Post quantization calibration\n",
        "ptq_model.apply(torch.quantization.disable_fake_quant)\n",
        "ptq_model.apply(torch.quantization.enable_observer)\n",
        "calibrate(ptq_model, train_loader, max_iteration=100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Disable observer and enable fake quantization to validate model with quantization error\n",
        "ptq_model.apply(torch.quantization.disable_observer)\n",
        "ptq_model.apply(torch.quantization.enable_fake_quant)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "with torch.no_grad():\n",
        "    ptq_model.eval()\n",
        "    ptq_model.cpu()\n",
        "\n",
        "    # The step below converts the model to an actual quantized model, which uses the quantized kernels.\n",
        "    ptq_model = quantizer.convert(ptq_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "ptq_model.encoder_stem_0_layers_0.weight().dtype"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Evaluation of the quantized model (PTQ) per tensor with CLE ...\")\n",
        "mIoU, fwIoU = evaluate(ptq_model, test_dataset, verbose=False, device=\"cpu\")\n",
        "print(f\"Validation mIoU: {mIoU:.2f}\")\n",
        "print(f\"Validation Frequence Weighted IoU: {fwIoU:.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### PTQ per channel without CLE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "with model_tracer():\n",
        "    model_copy = copy.deepcopy(model).eval()\n",
        "    model_copy = model.to(DEVICE)\n",
        "    dummy_input = torch.randn(1, 3, 256, 256)\n",
        "\n",
        "    # More information for QATQuantizer initialization, see `examples/quantization/qat.py`.\n",
        "    # We set 'override_qconfig_func' when initializing QATQuantizer to use fake-quantize to do post quantization.\n",
        "    quantizer = PostQuantizer(\n",
        "        model_copy,\n",
        "        dummy_input,\n",
        "        work_dir=\"out\",\n",
        "        config={\n",
        "            \"asymmetric\": True,\n",
        "            \"backend\": \"qnnpack\",\n",
        "            \"disable_requantization_for_cat\": True,\n",
        "            \"per_tensor\": False,\n",
        "            \"override_qconfig_func\": set_ptq_fake_quantize,\n",
        "        },\n",
        "    )\n",
        "    ptq_model = quantizer.quantize()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "ptq_model.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "activation = {}\n",
        "ptq_model = ptq_model.to(\"cpu\")\n",
        "\n",
        "\n",
        "def getActivation(name):\n",
        "    # the hook signature\n",
        "    def hook(model, input, output):\n",
        "        activation[name] = output.detach()\n",
        "\n",
        "    return hook\n",
        "\n",
        "\n",
        "# register forward hooks on the layers of choice\n",
        "h1 = ptq_model.encoder_stem_0_layers_0.register_forward_hook(\n",
        "    getActivation(\"encoder_stem_0_layers_0\")\n",
        ")\n",
        "\n",
        "activation_list = []\n",
        "for imgs, _ in tqdm(val_loader):\n",
        "    out = ptq_model(imgs)\n",
        "    activation_list.append(activation[\"encoder_stem_0_layers_0\"])\n",
        "h1.remove()\n",
        "activations = torch.cat(activation_list[:-1], dim=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plot_output_channel(activations)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "ptq_model = ptq_model.to(DEVICE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Post quantization calibration\n",
        "ptq_model.apply(torch.quantization.disable_fake_quant)\n",
        "ptq_model.apply(torch.quantization.enable_observer)\n",
        "calibrate(ptq_model, train_loader, max_iteration=100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Disable observer and enable fake quantization to validate model with quantization error\n",
        "ptq_model.apply(torch.quantization.disable_observer)\n",
        "ptq_model.apply(torch.quantization.enable_fake_quant)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "with torch.no_grad():\n",
        "    ptq_model.eval()\n",
        "    ptq_model.cpu()\n",
        "\n",
        "    # The step below converts the model to an actual quantized model, which uses the quantized kernels.\n",
        "    ptq_model = quantizer.convert(ptq_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "ptq_model.encoder_stem_0_layers_0.weight().dtype"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Evaluation of the quantized model (PTQ) per channel ...\")\n",
        "mIoU, fwIoU = evaluate(ptq_model, test_dataset, verbose=False, device=\"cpu\")\n",
        "print(f\"Validation mIoU: {mIoU:.2f}\")\n",
        "print(f\"Validation Frequence Weighted IoU: {fwIoU:.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### PTQ per channel with CLE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "with model_tracer():\n",
        "    model_copy = copy.deepcopy(model).eval()\n",
        "    model_copy = model.to(DEVICE)\n",
        "    dummy_input = torch.randn(1, 3, 256, 256)\n",
        "    model_copy = cross_layer_equalize(model_copy, dummy_input, DEVICE, cle_iters=10)\n",
        "\n",
        "    # More information for QATQuantizer initialization, see `examples/quantization/qat.py`.\n",
        "    # We set 'override_qconfig_func' when initializing QATQuantizer to use fake-quantize to do post quantization.\n",
        "    quantizer = PostQuantizer(\n",
        "        model_copy,\n",
        "        dummy_input,\n",
        "        work_dir=\"out\",\n",
        "        config={\n",
        "            \"asymmetric\": True,\n",
        "            \"backend\": \"qnnpack\",\n",
        "            \"disable_requantization_for_cat\": True,\n",
        "            \"per_tensor\": False,\n",
        "            \"override_qconfig_func\": set_ptq_fake_quantize,\n",
        "        },\n",
        "    )\n",
        "    ptq_model = quantizer.quantize()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "ptq_model.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "activation = {}\n",
        "ptq_model = ptq_model.to(\"cpu\")\n",
        "\n",
        "\n",
        "def getActivation(name):\n",
        "    # the hook signature\n",
        "    def hook(model, input, output):\n",
        "        activation[name] = output.detach()\n",
        "\n",
        "    return hook\n",
        "\n",
        "\n",
        "# register forward hooks on the layers of choice\n",
        "h1 = ptq_model.encoder_stem_0_layers_0.register_forward_hook(\n",
        "    getActivation(\"encoder_stem_0_layers_0\")\n",
        ")\n",
        "\n",
        "activation_list = []\n",
        "for imgs, _ in tqdm(val_loader):\n",
        "    out = ptq_model(imgs)\n",
        "    activation_list.append(activation[\"encoder_stem_0_layers_0\"])\n",
        "h1.remove()\n",
        "activations = torch.cat(activation_list[:-1], dim=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plot_output_channel(activations)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "ptq_model = ptq_model.to(DEVICE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Post quantization calibration\n",
        "ptq_model.apply(torch.quantization.disable_fake_quant)\n",
        "ptq_model.apply(torch.quantization.enable_observer)\n",
        "calibrate(ptq_model, train_loader, max_iteration=100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Disable observer and enable fake quantization to validate model with quantization error\n",
        "ptq_model.apply(torch.quantization.disable_observer)\n",
        "ptq_model.apply(torch.quantization.enable_fake_quant)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "with torch.no_grad():\n",
        "    ptq_model.eval()\n",
        "    ptq_model.cpu()\n",
        "\n",
        "    # The step below converts the model to an actual quantized model, which uses the quantized kernels.\n",
        "    ptq_model = quantizer.convert(ptq_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "ptq_model.encoder_stem_0_layers_0.weight().dtype"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Evaluation of the quantized model (PTQ) per channel with CLE ...\")\n",
        "mIoU, fwIoU = evaluate(ptq_model, test_dataset, verbose=False, device=\"cpu\")\n",
        "print(f\"Validation mIoU: {mIoU:.2f}\")\n",
        "print(f\"Validation Frequence Weighted IoU: {fwIoU:.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Summary of PTQ\n",
        "\n",
        "|Model  | Val mIoU | Val Frequency Weighted IoU |\n",
        "|-------|-------------------|----|\n",
        "|Original model (FP32) |  0.37  | 0.66 |\n",
        "|Per tensor quantization (INT8) | 0.29 | 0.16  |\n",
        "|Per tensor quantization + CLE (INT8) | 0.29 | 0.17  |\n",
        "|Per channel quantization (INT8) | 0.37 | 0.65  |\n",
        "|Per channel quantization + CLE (INT8) | 0.37 | 0.65  |\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Quantization aware training (QAT)\n",
        "QAT models the quantization noise source during training. This allows the model to find more optimal solutions than post-training quantization. However, the higher accuracy comes with the usual costs of neural network training, i.e., longer training times, need for labeled data and hyper-parameter search."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from tinynn.graph.quantization.quantizer import QATQuantizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "with model_tracer():\n",
        "    model_copy = copy.deepcopy(model).train()\n",
        "    model_copy = model.to(DEVICE)\n",
        "    dummy_input = torch.randn(1, 3, 256, 256)\n",
        "\n",
        "    # TinyNeuralNetwork provides a QATQuantizer class that may rewrite the graph for and perform model fusion for\n",
        "    # quantization. The model returned by the `quantize` function is ready for QAT.\n",
        "    # By default, the rewritten model (in the format of a single file) will be generated in the working directory.\n",
        "    # You may also pass some custom configuration items through the argument `config` in the following line. For\n",
        "    # example, if you have a QAT-ready model (e.g models in torchvision.models.quantization),\n",
        "    # then you may use the following line.\n",
        "    #   quantizer = QATQuantizer(model, dummy_input, work_dir='out', config={'rewrite_graph': False})\n",
        "    # Alternatively, if you have modified the generated model description file and want the quantizer to load it\n",
        "    # instead, then use the code below.\n",
        "    #     quantizer = QATQuantizer(\n",
        "    #         model, dummy_input, work_dir='out', config={'force_overwrite': False, 'is_input_quantized': None}\n",
        "    #     )\n",
        "    # The `is_input_quantized` in the previous line is a flag on the input tensors whether they are quantized or\n",
        "    # not, which can be None (False for all inputs) or a list of booleans that corresponds to the inputs.\n",
        "    # Also, we support multiple qschemes for quantization preparation. There are several common choices.\n",
        "    #   a. Asymmetric uint8. (default) config={'asymmetric': True, 'per_tensor': True}\n",
        "    #      The is the most common choice and also conforms to the legacy TFLite quantization spec.\n",
        "    #   b. Asymmetric int8. config={'asymmetric': True, 'per_tensor': False}\n",
        "    #      The conforms to the new TFLite quantization spec. In legacy TF versions, this is usually used in post\n",
        "    #      quantization. Compared with (a), it has support for per-channel quantization in supported kernels\n",
        "    #      (e.g Conv), while (a) does not.\n",
        "    #   c. Symmetric int8. config={'asymmetric': False, 'per_tensor': False}\n",
        "    #      The is same to (b) with no offsets, which may be used on some low-end embedded chips.\n",
        "    #   d. Symmetric uint8. config={'asymmetric': False, 'per_tensor': True}\n",
        "    #      The is same to (a) with no offsets. But it is rarely used, which just serves as a placeholder here.\n",
        "\n",
        "    quantizer = QATQuantizer(\n",
        "        model_copy,\n",
        "        dummy_input,\n",
        "        work_dir=\"out\",\n",
        "        config={\n",
        "            \"asymmetric\": True,\n",
        "            \"backend\": \"qnnpack\",\n",
        "            \"disable_requantization_for_cat\": True,\n",
        "            \"per_tensor\": False,\n",
        "        },\n",
        "    )\n",
        "    qat_model = quantizer.quantize()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "qat_model = qat_model.to(DEVICE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "EPOCHS = 5\n",
        "qat_optimizer = torch.optim.AdamW(\n",
        "    qat_model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY\n",
        ")\n",
        "qat_lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(qat_optimizer, EPOCHS)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE // 2)\n",
        "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE // 2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_losses = []\n",
        "val_losses = []\n",
        "ious = []\n",
        "fwious = []\n",
        "qat_model.to(DEVICE)\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    qat_model.train()\n",
        "\n",
        "    epoch_train_loss = 0.0\n",
        "    epoch_val_loss = 0.0\n",
        "    for imgs, masks in train_loader:\n",
        "        imgs = imgs.to(DEVICE)\n",
        "        masks = masks.to(DEVICE).float()\n",
        "        outputs = qat_model(imgs)\n",
        "\n",
        "        loss = criterion(outputs, masks)\n",
        "        loss.backward()\n",
        "        qat_optimizer.step()\n",
        "\n",
        "        epoch_train_loss += loss.item()\n",
        "\n",
        "    qat_lr_scheduler.step()\n",
        "\n",
        "    qat_model.eval()\n",
        "    iou = 0.0\n",
        "    fwiou = 0.0\n",
        "    for imgs, masks in val_loader:\n",
        "        imgs = imgs.to(DEVICE)\n",
        "        masks = masks.to(DEVICE).float()\n",
        "        outputs = qat_model(imgs)\n",
        "        loss = criterion(outputs, masks)\n",
        "\n",
        "        preds = torch.argmax(outputs, dim=1).cpu().numpy()\n",
        "        gt = torch.argmax(masks, dim=1).cpu().numpy()\n",
        "\n",
        "        iou_scores, class_frequencies, _fwiou = calculate_frequency_weighted_iou(\n",
        "            gt.flatten(), preds.flatten(), NUM_CLASSES\n",
        "        )\n",
        "\n",
        "        epoch_val_loss += loss.item()\n",
        "        iou += np.mean(iou_scores)\n",
        "        fwiou += _fwiou\n",
        "\n",
        "    iou = iou / len(val_loader)\n",
        "    fwiou = fwiou / len(val_loader)\n",
        "    epoch_train_loss = epoch_train_loss / len(train_loader)\n",
        "    epoch_val_loss = epoch_val_loss / len(val_loader)\n",
        "\n",
        "    print(\n",
        "        f\"[Epoch {epoch + 1} / {EPOCHS}] [train loss: {epoch_train_loss:.3f}] [val loss: {epoch_val_loss:.3f}] [val mIoU: {iou:.3f}] [val fwIoU: {fwiou:.3f}]\"\n",
        "    )\n",
        "\n",
        "    train_losses.append(epoch_train_loss)\n",
        "    val_losses.append(epoch_val_loss)\n",
        "    ious.append(iou)\n",
        "    fwious.append(fwiou)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "qat_model.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "activation = {}\n",
        "qat_model = qat_model.to(\"cpu\")\n",
        "\n",
        "\n",
        "def getActivation(name):\n",
        "    # the hook signature\n",
        "    def hook(model, input, output):\n",
        "        activation[name] = output.detach()\n",
        "\n",
        "    return hook\n",
        "\n",
        "\n",
        "# register forward hooks on the layers of choice\n",
        "h1 = qat_model.encoder_stem_0_layers_0.register_forward_hook(\n",
        "    getActivation(\"encoder_stem_0_layers_0\")\n",
        ")\n",
        "\n",
        "activation_list = []\n",
        "for imgs, _ in tqdm(val_loader):\n",
        "    out = qat_model(imgs)\n",
        "    activation_list.append(activation[\"encoder_stem_0_layers_0\"])\n",
        "h1.remove()\n",
        "activations = torch.cat(activation_list[:-1], dim=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plot_output_channel(activations)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "with torch.no_grad():\n",
        "    qat_model.eval()\n",
        "    qat_model.cpu()\n",
        "\n",
        "    # The step below converts the model to an actual quantized model, which uses the quantized kernels.\n",
        "    qat_model = quantizer.convert(qat_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "qat_model.encoder_stem_0_layers_0.weight().dtype"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Evaluation of the quantized model (QAT) per channel ...\")\n",
        "mIoU, fwIoU = evaluate(qat_model, test_dataset, verbose=False, device=\"cpu\")\n",
        "print(f\"Validation mIoU: {mIoU:.2f}\")\n",
        "print(f\"Validation Frequence Weighted IoU: {fwIoU:.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Project summary\n",
        "|Model  | Val mIoU | Val Frequency Weighted IoU |\n",
        "|-------|-------------------|----|\n",
        "|Original model (FP32) |  0.37  | 0.66 |\n",
        "|Per tensor quantization (INT8) | 0.29 | 0.16  |\n",
        "|Per tensor quantization + CLE (INT8) | 0.29 | 0.17  |\n",
        "|Per channel quantization (INT8) | 0.37 | 0.65  |\n",
        "|Per channel quantization + CLE (INT8) | 0.37 | 0.65  |\n",
        "|Per channel QAT (INT8) | 0.35 | 0.62|"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.19"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
